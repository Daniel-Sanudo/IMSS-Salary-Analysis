{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040a9cfd",
   "metadata": {},
   "source": [
    "# Dataset sourcing\n",
    "\n",
    "## API method\n",
    "\n",
    "Salary information from years 1997 to 2018 can be accessed using the IMSS api endpoint with the following structure:\n",
    "\n",
    "```\n",
    "http://datos.imss.gob.mx/api/action/datastore/search.json?resource_id={resource_id}\n",
    "```\n",
    "\n",
    "Where __resource_id__ is the identifier for the files stored for each year. \n",
    "\n",
    "The API method get is not reliable since some of the endpoints return blank jsons or have not been uploaded.\n",
    "\n",
    "## Web scraping method\n",
    "\n",
    "Salary information from years 2019 to the current year (2022) is not yet available in the API endpoint, therefore it will have to be downloaded using a webscraper that can access the following url:\n",
    "\n",
    "```\n",
    "http://datos.imss.gob.mx/dataset/asg-{year}\n",
    "```\n",
    "\n",
    "Where __year__ corresponds to the target year.\n",
    "\n",
    "After accesing the target url, the next step is to gather all the relative urls for each of the 12 files per year.\n",
    "\n",
    "The HTML element for each of the target files follows this structure\n",
    "\n",
    "```\n",
    "<a href=\"/dataset/asg{year}/resource/asg-{year}-{month}-{day}\" class=\"heading\" title=\"asg-{year}-{month}-{day}\" property=\"dcat:accessURL\">asg-{year}-{month}-{day}<span class=\"format-label\" property=\"dc:format\" data-format=\"csv\">csv</span></a>\n",
    "```\n",
    "\n",
    "Where __year__ corresponds to the target year, __month__ is the target month for the specified year, __day__ is the last day for the target month and year.\n",
    "\n",
    "After joining the base url with the relative url extracted from the html elements, the result would be the following:\n",
    "\n",
    "```\n",
    "http://datos.imss.gob.mx/dataset/asg2020/resource/asg-{year}-{month}-{day}\n",
    "```\n",
    "\n",
    "Inside this url, the next step is to locate the following html element:\n",
    "\n",
    "```\n",
    "<a href=\"/node/{id}/download\" class=\"btn-primary btn\"><i class=\"icon-large icon-download\"></i> Descargar</a>\n",
    "```\n",
    "\n",
    "Where __id__ is the number of the csv file, usually in a 4 digit format. Joining this url with the base url gives the final path to download the required csv.\n",
    "\n",
    "Each of these csv files is around 400 megabytes. Assuming all files for the 26 years have the same filesize, it would be a total size of 120 gigabytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec064748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter, BeautifulSoup, and Pandas\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from selenium import webdriver\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import requests\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fcf9a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_year(target_year,target_month='All'):\n",
    "    \n",
    "    # Determine directory path\n",
    "    download_path = os.path.join(os.getcwd(),'IMSS_Files')\n",
    "    \n",
    "    # Create download directory in case it doesnt exist\n",
    "    os.makedirs(download_path,\n",
    "               exist_ok = True)\n",
    "    \n",
    "    # Create options chrome options instance\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Create options dictionary\n",
    "    prefs = {\"download.default_directory\" : f'{download_path}',\n",
    "            \"download.directory_upgrade\": \"true\",\n",
    "            \"download.prompt_for_download\": \"false\",\n",
    "            \"disable-popup-blocking\": \"false\"}\n",
    "    \n",
    "    # Add options dictionary to options instance\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    # Install chrome driver manager\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    \n",
    "    # Initiate automated browser with preferred options\n",
    "    browser = Browser('chrome', **executable_path, headless=True, options=options)\n",
    "    \n",
    "    scrape_month(browser,target_year,target_month, download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d29bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_month(browser,year,month, path):\n",
    "    \n",
    "    # Get the list of files that were already downloaded in the download path\n",
    "    previously_downloaded_files = os.listdir(path)\n",
    "    \n",
    "    # Define function to verify if all downloads were completed\n",
    "    def download_status():\n",
    "        # Create loop to verify the download status of the file without having to access the\n",
    "        # shadow-root of chrome downloads\n",
    "        while True: \n",
    "            # Wait 5 seconds to confirm the newly downloaded file shows up\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Get a list of all the files in the download path including the new ones\n",
    "            all_downloaded_files = os.listdir(path)\n",
    "            \n",
    "            # Remove the files that were previously downloaded from the list\n",
    "            new_downloaded_files = [x for x in all_downloaded_files \n",
    "                                    if x not in previously_downloaded_files]\n",
    "            \n",
    "            # Get a list of the files that are pending download\n",
    "            pending_downloads = [x for x in new_downloaded_files\n",
    "                                    if 'crdownload' in x]\n",
    "            \n",
    "            # Verify if all files finished downloading to quit the browser \n",
    "            if len(pending_downloads) == 0:\n",
    "                break\n",
    "    \n",
    "    # Set the target url according to the year argument\n",
    "    target_url = f'http://datos.imss.gob.mx/dataset/asg-{year}'\n",
    "    \n",
    "    # Acess the url for the target year\n",
    "    browser.visit(target_url)\n",
    "    \n",
    "    # Validate if there's a specific target month or if all files will be downloaded\n",
    "    if month == 'All':\n",
    "        \n",
    "        # Create list to store the list of extracted urls as strings\n",
    "        links_for_each_month = []\n",
    "        \n",
    "        # Loop through the urls that match the search text asg-{year}\n",
    "        for link in browser.links.find_by_partial_text(f'asg-{year}'):\n",
    "            \n",
    "            # Append the link as a string to avoid stale element error\n",
    "            links_for_each_month.append(str(link['href']))\n",
    "        \n",
    "        # Loop through the link list given by links_for_each_month\n",
    "        for link in links_for_each_month:\n",
    "            \n",
    "            # Visit the new url\n",
    "            browser.visit(link)\n",
    "            \n",
    "            # Look for the url that includes node in its path to find the csv\n",
    "            for download_link in browser.links.find_by_partial_href('node'):\n",
    "                \n",
    "                # Store the link as a string to avoid stale element error\n",
    "                csv_link = str(download_link['href'])\n",
    "            \n",
    "            # Visit the path with link which will be downloaded\n",
    "            browser.visit(csv_link)\n",
    "            \n",
    "            # Find the download button and click it\n",
    "            browser.links.find_by_partial_href('download').click()\n",
    "            \n",
    "            # Wait for the file to finish downloading before moving to the next one\n",
    "            download_status()\n",
    "            \n",
    "        # Close the browser\n",
    "        browser.quit()\n",
    "        \n",
    "    # Download only the csv file for the target month\n",
    "    else:\n",
    "        \n",
    "        # Cast month as a string\n",
    "        month = str(month)\n",
    "        \n",
    "        # Verify that the target month begins with 0 in case it's below 10\n",
    "        if month[0] != 0 and len(month)<2:\n",
    "            \n",
    "            # Add a 0 before the target month\n",
    "            month = f'0{month}'\n",
    "            \n",
    "        # Append the link as a string to avoid stale element error\n",
    "        link_for_target_month = str(browser.links.find_by_partial_text(f'asg-{year}-{month}')['href'])\n",
    "        \n",
    "        # Visit the new url\n",
    "        browser.visit(link_for_target_month)   \n",
    "        \n",
    "        # Find the download button and click it\n",
    "        browser.links.find_by_partial_href('download').click()\n",
    "        \n",
    "        # Track the download status\n",
    "        download_status()\n",
    "        \n",
    "        # Close the browser\n",
    "        browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0df08f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_year(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd94f988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
